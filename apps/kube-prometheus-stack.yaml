apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: kube-prometheus-stack
  namespace: argocd
spec:
  generators:
  - matrix:
      generators:
        - clusters: {}
        - git:
            repoURL: https://github.com/mogopz/argocd-infra
            revision: HEAD
            files:
            - path: config/{{name}}/kube-prometheus-stack.yaml
  template:
    metadata:
      name: kube-prometheus-stack
      namespace: argocd
    spec:
      project: default
      source:
        repoURL: https://github.com/mogopz/argocd-infra
        targetRevision: HEAD
        path: charts/kube-prometheus-stack
        helm:
          values: |
            defaultRules:
              rules:
                etcd: false
                kubeScheduler: false

            kubeControllerManager:
              enabled: false
            kubeEtcd:
              enabled: false
            kubeScheduler:
              enabled: false

            kubeStateMetrics:
              enabled: true

            prometheus:
              serviceAccount:
                create: true
                annotations:
                  eks.amazonaws.com/role-arn: "some-arn"
                  eks.amazonaws.com/sts-regional-endpoints: "true"

              prometheusSpec:
                query:
                  maxSamples: 20000000

                retention: 30d
                retentionSize: 170GB

            alertmanager:
              config:
                global:
                  resolve_timeout: 5m
                route:
                  group_by: ['cluster', 'alertname', 'service']
                  group_wait: 30s
                  group_interval: 5m
                  repeat_interval: 3h
                  receiver: opsgenie
                  routes:
                  - receiver: heartbeats
                    matchers:
                      - alertname = "Watchdog"
                    group_interval: 1m
                    repeat_interval: 1m
                  - receiver: opsgenie
                    matchers:
                      - severity = "critical"
                  - receiver: opsgenie
                    matchers:
                      - alertname = "TargetDown"
                  - receiver: 'null'
                    matchers:
                      - alertname =~ "NodeFilesystemFilesFillingUp|NodeFilesystemSpaceFillingUp|CPUThrottlingHigh|KubeCPUOvercommit|KubeletTooManyPods"
                  - receiver: 'null'
                    matchers:
                      - alertname = "InfoInhibitor"
                  - receiver: opsgenie-low-priority
                    matchers:
                      - severity =~ "info|warning"

                inhibit_rules:
                  - source_matchers:
                      - severity = critical
                    target_matchers:
                      - severity = warning
                    equal: ['alertname', 'cluster', 'service']

              alertmanagerSpec:
                resources:
                  requests:
                    cpu: 50m
                    memory: 128Mi
                  limits:
                    memory: 256Mi

                storage:
                  volumeClaimTemplate:
                    spec:
                      storageClassName: gp2
                      accessModes: ["ReadWriteOnce"]
                      resources:
                        requests:
                          storage: 1Gi

            prometheus-node-exporter:
              priorityClassName: system-node-critical

              updateStrategy:
                type: RollingUpdate
                rollingUpdate:
                  maxUnavailable: 2

              resources:
                limits:
                  memory: 128Mi
                requests:
                  cpu: 50m
                  memory: 64Mi

            grafana:
              enabled: true

              headlessService: true

              replicas: 2
              podDisruptionBudget:
                maxUnavailable: 1

              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  memory: 256Mi

              serviceAccount:
                create: true
                annotations:
                  eks.amazonaws.com/role-arn: "some-arn"
                  eks.amazonaws.com/sts-regional-endpoints: "true"

              serviceMonitor:
                enabled: true

              image:
                repository: grafana/grafana
                tag: "10.0.1"

              plugins:
                - grafana-worldmap-panel
                - grafana-piechart-panel
                - grafana-clock-panel
                - blackmirror1-singlestat-math-panel
                - grafana-github-datasource

              securityContext:
                fsGroup: 472
                runAsGroup: 472
                runAsUser: 472

              service:
                type: ClusterIP

              sidecar:
                datasources:
                  enabled: true
                  defaultDatasourceEnabled: false
                dashboards:
                  defaultFolderName: kubernetes
                resources:
                  requests:
                    cpu: 100m
                    memory: 128Mi
                  limits:
                    memory: 256Mi
      destination:
        server: "{{server}}"
        namespace: monitoring
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
          - ApplyOutOfSyncOnly=true
          - ServerSideApply=true
          - FailOnSharedResource=true